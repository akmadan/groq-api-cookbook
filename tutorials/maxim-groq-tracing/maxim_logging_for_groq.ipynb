{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tracing & Evaluation for Groq based Agents using Maxim AI\n",
        "\n",
        "Learn how to integrate Maxim observability with the Groq SDK. \n",
        "\n",
        "When you use Groq with Maxim instrumentation, the following information is automatically captured for each API call:\n",
        "- Request Details: Model name, temperature, max tokens, and all other parameters\n",
        "- Message History: Complete conversation context including system and user messages\n",
        "- Response Content: Full assistant responses and metadata\n",
        "- Usage Statistics: Input tokens, output tokens, total tokens consumed\n",
        "- Cost Tracking: Estimated costs based on Groqâ€™s pricing\n",
        "- Error Handling: Any API errors or failures with detailed context\n",
        "- Node Level Evaluations\n",
        "- Get Real Time Alerts (Slack, PagerDuty, etc.)\n",
        "\n",
        "Link to Docs - https://getmax.im/HIF14Di"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies & Set environment variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "\n",
        "'''\n",
        "%pip install groq\n",
        "%pip install maxim-py\n",
        "'''\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"MAXIM_API_KEY\"] = \"YOUR_MAXIM_API_KEY\"\n",
        "os.environ[\"MAXIM_LOG_REPO_ID\"] = \"YOUR_MAXIM_LOG_REPO_ID\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECs9RZXlE4sp"
      },
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "\n",
        "from maxim.logger.groq import instrument_groq\n",
        "from maxim import Config, Maxim\n",
        "from maxim import logger\n",
        "from maxim.logger import LoggerConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRmRYFqSFRDe"
      },
      "source": [
        "## Create a Maxim Logger & Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnoyfPMAFQKf",
        "outputId": "85ed1cb4-b551-43a0-c361-468cbe2bea20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[MaximSDK] Initializing Maxim AI(v3.9.12)\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-5-308030384.py:1: DeprecationWarning: This class will be removed in a future version. Use {} which is TypedDict.\n",
            "  maxim = Maxim(Config(api_key= MAXIM_API_KEY))\n",
            "/tmp/ipython-input-5-308030384.py:3: DeprecationWarning: This class will be removed in a future version. Use LoggerConfigDict instead.\n",
            "  logger = maxim.logger(LoggerConfig(id=MAXIM_LOG_REPO_ID))\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "maxim_api_key = os.environ.get(\"MAXIM_API_KEY\")\n",
        "maxim_log_repo_id = os.environ.get(\"MAXIM_LOG_REPO_ID\")\n",
        "\n",
        "maxim = Maxim(Config(api_key=maxim_api_key))\n",
        "\n",
        "logger = maxim.logger(LoggerConfig(id=maxim_log_repo_id))\n",
        "\n",
        "instrument_groq(logger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you have Maxim configured, you can start logging your Groq model interactions seamlessly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgQOUb3cDrN8"
      },
      "source": [
        "## Simple Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QoOyXU64B_M7"
      },
      "outputs": [],
      "source": [
        "client = Groq()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fvWbpCueCQNP"
      },
      "outputs": [],
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    model=\"llama-3.3-70b-versatile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOoTM_6eDisw",
        "outputId": "f613e90c-ebef-4fa8-c127-66bf3ad7acb0"
      },
      "outputs": [],
      "source": [
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaxo0GVzDo0b"
      },
      "source": [
        "## Streaming Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-AAjJxdDjrE",
        "outputId": "382b8813-2cfe-46ba-ea35-630ffd296ffd"
      },
      "outputs": [],
      "source": [
        "stream = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "    temperature=0.5,\n",
        "\n",
        "    max_completion_tokens=1024,\n",
        "\n",
        "    top_p=1,\n",
        "\n",
        "    stop=None,\n",
        "\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBQwao4qD6hv"
      },
      "source": [
        "## Async Chat Completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd9oTtWsEJeL"
      },
      "outputs": [],
      "source": [
        "from groq import AsyncGroq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EvQYxpVDzNE",
        "outputId": "c56a800d-3360-4130-ffec-7c3b656301c2"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "from groq import AsyncGroq\n",
        "\n",
        "async def main():\n",
        "    client = AsyncGroq()\n",
        "\n",
        "    chat_completion = await client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "            # Set a user message for the assistant to respond to.\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Explain the importance of fast language models\",\n",
        "            }\n",
        "        ],\n",
        "\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "        temperature=0.5,\n",
        "\n",
        "        max_completion_tokens=1024,\n",
        "        top_p=1,\n",
        "\n",
        "        stop=None,\n",
        "\n",
        "        stream=False,\n",
        "    )\n",
        "\n",
        "    # Print the completion returned by the LLM.\n",
        "    print(chat_completion.choices[0].message.content)\n",
        "\n",
        "await main() # Use asyncio.run if not working in jupyter environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2TW14ynEetv"
      },
      "source": [
        "## Async Completion with Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8e9kOAGEQlC",
        "outputId": "20ad1161-ae65-4aa1-d751-00c0de4d748f"
      },
      "outputs": [],
      "source": [
        "async def main():\n",
        "    client = AsyncGroq()\n",
        "\n",
        "    stream = await client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\"\n",
        "            },\n",
        "            # Set a user message for the assistant to respond to.\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Explain the importance of fast language models\",\n",
        "            }\n",
        "        ],\n",
        "\n",
        "        # The language model which will generate the completion.\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "\n",
        "        temperature=0.5,\n",
        "\n",
        "        max_completion_tokens=1024,\n",
        "\n",
        "        top_p=1,\n",
        "\n",
        "        stop=None,\n",
        "\n",
        "        # If set, partial message deltas will be sent.\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    # Print the incremental deltas returned by the LLM.\n",
        "    async for chunk in stream:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "await main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Logs on Maxim Dashboard\n",
        "\n",
        "![Maxim Dashboard](images/groq_fin_maxim.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we saw how you can use Maxim to log your Groq model interactions.\n",
        "We can get the following info on the Maxim dashboard:\n",
        "- View detailed logs of all your LLM interactions\n",
        "- Inspect request and response payloads\n",
        "- Monitor model usage and performance metrics\n",
        "- Track Tool / Function Calls\n",
        "- Run Node Level Evals & Get Real Time Alerts (Slack, PagerDuty, etc.)\n",
        "\n",
        "For more information, check this [cookbook](https://www.getmaxim.ai/docs/cookbooks/integrations/groq)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
